{\rtf1\ansi\ansicpg1252\cocoartf2757
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\ftech\fcharset77 Symbol;\f2\froman\fcharset0 TimesNewRomanPSMT;
\f3\fswiss\fcharset0 Helvetica-Bold;\f4\fmodern\fcharset0 CourierNewPSMT;\f5\fnil\fcharset0 Menlo-Regular;
\f6\fnil\fcharset0 Menlo-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;\red15\green55\blue78;
\red56\green101\blue115;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\csgray\c0\c0;\cssrgb\c5882\c27843\c38039;
\cssrgb\c27451\c47059\c52549;}
\margl1440\margr1440\vieww18100\viewh16680\viewkind0
\deftab720
\pard\pardeftab720\sl613\sa106\partightenfactor0

\f0\fs53\fsmilli26667 \cf2 \cb3 \expnd0\expndtw0\kerning0
OLLAMA\
\pard\pardeftab720\li960\fi-480\sl368\sa213\partightenfactor0

\f1\fs32 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Ollama is an open-source LLM model manager. It provides a unified front-end for deploying 50+ popular open-source LLMs (including various flavors of llama2 and Mistral), simplifying the experience. \
\pard\pardeftab720\sl368\partightenfactor0

\f3\b \cf2 Links:
\f0\b0 \
\pard\pardeftab720\li960\fi-480\sl368\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 {\field{\*\fldinst{HYPERLINK "https://ollama.ai/"}}{\fldrslt 
\f0\fs32 \cf5 \ul \ulc5 https://ollama.ai}}
\f0\fs32  \

\f1 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Great resources for general ollama use: {\field{\*\fldinst{HYPERLINK "https://github.com/jmorganca/ollama"}}{\fldrslt \cf5 \ul \ulc5 https://github.com/jmorganca/ollama}} \

\f1 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Ollama API: {\field{\*\fldinst{HYPERLINK "https://github.com/ollama/ollama/blob/main/docs/api.md"}}{\fldrslt \cf5 \ul \ulc5 https://github.com/ollama/ollama/blob/main/docs/api.md}} \

\f1 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Overview and setup: {\field{\*\fldinst{HYPERLINK "https://klu.ai/glossary/ollama"}}{\fldrslt \cf5 \ul \ulc5 https://klu.ai/glossary/ollama}} \
\pard\pardeftab720\li960\fi-480\sl368\sa213\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Models/tags: {\field{\*\fldinst{HYPERLINK "https://ollama.ai/library"}}{\fldrslt \cf5 \ul \ulc5 https://ollama.ai/library}} \
\pard\pardeftab720\sl368\partightenfactor0

\f3\b \cf2 Install
\f0\b0 :\
\pard\pardeftab720\li960\fi-480\sl368\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 OSX or Linux\

\f1 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 \ul Local\ulnone :\
\pard\pardeftab720\li1920\fi-480\sl368\partightenfactor0

\f4 \cf2 o
\f2\fs18\fsmilli9333 \'a0\'a0 
\f0\fs32 Download {\field{\*\fldinst{HYPERLINK "https://ollama.ai/"}}{\fldrslt \cf5 \ul \ulc5 installer}}\cf5 \ul \ulc5  \cf2 \ulnone and run\
\pard\pardeftab720\li960\fi-480\sl368\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 \ul ARCC\ulnone :\
\pard\pardeftab720\li1920\fi-480\sl368\sa213\partightenfactor0

\f4 \cf2 o
\f2\fs18\fsmilli9333 \'a0\'a0 
\f0\fs32 Download or run 
\f5 curl https://ollama.ai/install.sh | sh
\f0 \
\pard\pardeftab720\sl368\partightenfactor0

\f3\b \cf2 Run:
\f0\b0 \
\pard\pardeftab720\li960\fi-480\sl368\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f5\fs32 ollama run <MODEL_NAME >:<TAGS>
\f0 \

\f1 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Example; to pass a tag calling the 70 billion parameter llama2 model:\
\pard\pardeftab720\li1920\fi-480\sl368\sa213\partightenfactor0

\f4 \cf2 o
\f2\fs18\fsmilli9333 \'a0\'a0 
\f5\fs32 ollama run llama2:70b
\f0 \
\pard\pardeftab720\sl368\partightenfactor0

\f3\b \cf2 Pull a model:
\f0\b0 \
\pard\pardeftab720\li960\fi-480\sl368\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Pull a model into local, without running it \'a0\

\f1 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f5\fs32 ollama pull <MODEL_NAME>:<TAGS>
\f0 \
\pard\pardeftab720\sl368\partightenfactor0

\f3\b \cf2 \'a0
\f0\b0 \
\pard\pardeftab720\sl368\sa213\partightenfactor0

\f3\b \cf2 Copy a model:
\f0\b0 \
\pard\pardeftab720\li960\fi-480\sl368\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Interacting with an LLM necessarily alters its training weights. We suggest copying the base model \

\f1 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 To copy a model:
\f5  ollama cp <MODEL_NAME>:<TAGS> <NEW_MODEL_NAME>
\f0 \
\pard\pardeftab720\li960\fi-480\sl368\sa213\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 To run a copy: 
\f5 ollama run <NEW_MODEL_NAME>
\f0 \
\pard\pardeftab720\sl368\partightenfactor0

\f3\b \cf2 Remove a model:
\f0\b0 \
\pard\pardeftab720\li960\fi-480\sl368\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Pull a model into local, without running it \'a0\

\f1 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f5\fs32 ollama rm <MODEL_NAME>:<TAGS>
\f0 \
\pard\pardeftab720\sl368\sa213\partightenfactor0

\f5 \cf2 \'a0
\f0 \
\pard\pardeftab720\sl368\sa213\partightenfactor0

\f3\b \cf2 Restore a mode to base:
\f0\b0 \
\pard\pardeftab720\li960\fi-480\sl368\partightenfactor0

\f1 \cf2 \'e1
\f2\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0 
\f5\fs32 ollama restore --model <MODEL_NAME>
\f0 \
\pard\pardeftab720\sl368\partightenfactor0

\f6\b \cf2 \'a0
\f0\b0 \
\pard\pardeftab720\sl368\partightenfactor0

\f3\b \cf2 Stop server:
\f0\b0 \
\pard\pardeftab720\fi960\sl368\partightenfactor0

\f5 \cf2 Pgrep ollama
\f0 \

\f5 >> <PROCESS_ID>>
\f0 \
\pard\pardeftab720\fi960\sl368\sa213\partightenfactor0

\f5 \cf2 Kill <PROCESS_ID>
\f0 \
\pard\pardeftab720\sl368\sa213\partightenfactor0
\cf2 \'a0\
}