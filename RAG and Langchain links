# RETRIEVAL-AUGMENTED GENERATION (RAG) & LANGCHAIN
In lieu of finetuning an LLM, which can be extremely time- and resource-consuming, an alternative method of document-specific LLM query is emerging. Retrieval-augmented generation (RAG) is essentially enabling the use of data outside of an LLMâ€™s immediate training set before generating a response. 
Langchain is a Python package. Some links:

-	RAG overview: https://aws.amazon.com/what-is/retrieval-augmented-generation/ 
-	A complete guide to langchain: https://www.sitepoint.com/langchain-python-complete-guide/ 
-	Simple ollama/langchain tutorial: https://github.com/jmorganca/ollama/blob/main/docs/tutorials/langchainpy.md 
-	Blog entry: prompt-querying knowledge graphs: https://medium.com/gopenai/llm-ontology-prompting-for-knowledge-graph-extraction-efdcdd0db3a1 
-	Langchain and knowledge graphs in more detail: https://blog.langchain.dev/constructing-knowledge-graphs-from-text-using-openai-functions/
-	Langchain document loaders: https://python.langchain.com/docs/integrations/document_loaders/ 
